# AI for Everyone

https://www.coursera.org/learn/ai-for-everyone

## Machine learning

Supervised learning learns mappings of A->B (input to output)

LLMs are built by using supervised learning (A->B) to repeatedly predict subsequent words. E.g. My favorite drink is lychee bubble tea.

Input: My favorite drink; Output: is

Input: My favorite drink is; Output: lychee

Input: My favorite drink is lychee; Output: bubble

Input: My favorite drink is lychee bubble; Output: tea

Input: My favorite drink is lychee bubble tea; Output: NA

When we train a very large AI system on a lot of data (100s billions of words or more) we get an LLM like ChatGPT.

Supervised learning took off recently in the 2010s because of the increase in 2 variables. First, tons of digital data are now available in many fields. Second, newer models have better performance with that increasing daata.

Specifically traditional AI flattens off quickly. Small neural nets continue to improve with a little more data. Medium NNs improve with even more data. Large NNs continue to improve.
## Glossary

* **ANI** Artificial narrow intelligense like smart speaker, self-driving car, web search
* **Generative AI** Generative artificial intelligence like ChatGPT, Bard, Midjourney, DALL-E
* **AGI** Artificial general intelligence. Anything a human can do and more. Andrew says this is decades off. I agree.
* **Machine learning**
* **LLM** Large language model uses supervised learning (A->B) to predict the next word
* **Supervised learning** Learns a mapping of A -> B like email -> spam (0/1); audio -> text transcripts; English -> Chinese; ad, user info -> click (0/1); image, radar info -> position of cars, obstacles, etc.

AI serves to impact most fields
